{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection Pipeline  Project Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 1: Decide which website you are going to collect data from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I have chosen to collect data from the book retailer Waterstones website. This site was chosen because it has useful data about books that I could collect.\n",
    "\n",
    "* The aim of the project would be to collect relevant data about books in the the bestsellers category.\n",
    "\n",
    "* Wew will be using the Selenium Chrome Webdriver written in Python for the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 2: Prototype finding the individual page for each entry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Create a Scraper Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Necessary Libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import time\n",
    "import uuid\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "s = Service('C:/Users/preco/Anaconda3/Library/bin/chromedriver.exe')\n",
    "\n",
    "options = Options()\n",
    "options.add_argument('--headless') \n",
    "\n",
    "options.add_argument(\"--window-size=1920x1080\")\n",
    "options.add_argument(\"user-agent=Mozilla/5.0 (Windows Phone 10.0; Android 4.2.1; Microsoft; Lumia 640 XL LTE) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/42.0.2311.135 Mobile Safari/537.36 Edge/12.10166\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This class contains the methods used to scrape data from your chosen website.\n",
    "\n",
    "* We will then navigate the website and get the required data. \n",
    "\n",
    "* We also initialise an instance of this class and use it to scrape the website selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    \n",
    "    def __init__(self, url: str='https://www.waterstones.com' ): \n",
    "        '''\n",
    "        We will create a class called Scraper which will contain the methods used to scrape data from your chosen website\n",
    "        \n",
    "        '''\n",
    "        self.driver = webdriver.Chrome(service=s, options=options) \n",
    "        self.driver.get(url)\n",
    "    \n",
    "    # Define a method that clicks on accept cookies\n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        \n",
    "        '''\n",
    "        We will Define a method that finds and clicks on accept cookies button. \n",
    "        If the cookies button is not found:\n",
    "        Return:\n",
    "            'No Cookies Found'\n",
    "\n",
    "        We will also add a timer which will allow 5 seconds for the website to load the webpage, look for the accept cookies button and  click on it.\n",
    "        '''\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "        except TimeoutException:\n",
    "            print('No Cookies Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Scraper_project()\n",
    "bot.accept_cookies()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2 and 3: Create Different Methods to Navigate the Webpage and, Implementing a method to bypass cookies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* I will implement a method that takes us to the page that contains books for the best sellers category. \n",
    "\n",
    "* But first, let use create a method which clicks on the accept cookies button as this frame pops up immediately the site is opened.\n",
    "\n",
    "* This is indeed Task 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    def __init__(self, url: str = 'https://www.waterstones.com' ):\n",
    "        self.driver = webdriver.Chrome(service=s, options=options) \n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        \n",
    "        '''\n",
    "        We will Define a method that finds and clicks on accept cookies button. \n",
    "        If the cookies button is not found:\n",
    "        Return:\n",
    "            'No Cookies Found'\n",
    "\n",
    "        We will also add a timer which will allow 1 seconds for the website to load the webpage, look for the accept cookies button and  click on it.\n",
    "        '''\n",
    "        time.sleep(1)\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "        except TimeoutException:\n",
    "            print('No Cookies Found')\n",
    "    \n",
    "    # We will define a method that click on the see more button of the best sellers and allows us see the books in this category\n",
    "    def bestsellers(self, xpath: str='/html/body/div[1]/div[2]/div[3]/header[1]/a'):\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() # The website has no frame. SO it clicks on  the accept cookies button directly.\n",
    "        except TimeoutException: #if the driver does not find the xpath, then print No cookies found\n",
    "            print('Not Found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Scraper_project()\n",
    "bot.accept_cookies()\n",
    "bot.bestsellers()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Creating a method to get the links to each page where details about the bestselling books can be found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    def __init__(self, url: str = 'https://www.waterstones.com' ): \n",
    "        self.driver = webdriver.Chrome(service=s, options=options) \n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        time.sleep(1)\n",
    "        self.cookies_button = self.driver.find_element(By.XPATH, xpath)\n",
    "        return self.cookies_button\n",
    "\n",
    "    def accept_cookies_click(self):\n",
    "        return self.accept_cookies().click()\n",
    "\n",
    "    \n",
    "    def bestsellers(self, xpath: str='//*[@id=\"pagesmain\"]/div[3]/header[1]/a'):\n",
    "        '''\n",
    "        We will define a method that click on the see more button of the best sellers,\n",
    "        and allows us view the books in this category.\n",
    "         \n",
    "        If this button is not found:\n",
    "        \n",
    "        Return:\n",
    "            'Not Found'\n",
    "        '''\n",
    "        try:\n",
    "            #self.driver.find_element(By.XPATH, xpath).click()\n",
    "            self.best = self.driver.find_element(By.XPATH, xpath)\n",
    "            self.best.click()\n",
    "        except TimeoutException:\n",
    "            print('Not Found')    \n",
    "    \n",
    "    def find_container(self, xpath: str = '//div[@class=\"search-results-list\"]'):\n",
    "        '''\n",
    "        This method finds the container containing all the data about the books in the bestseller category.\n",
    "        '''\n",
    "        return self.driver.find_element(By.XPATH, xpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"79f6f78900889101778ab680f0f79bc7\", element=\"4dfca5cd-6e36-49b5-94d1-314db143c3bf\")>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bot = Scraper_project()\n",
    "bot.accept_cookies()\n",
    "bot.accept_cookies_click()\n",
    "bot.bestsellers()\n",
    "bot.find_container()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.remote.webelement.WebElement"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "container = bot.find_container()\n",
    "type(container)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links(self):\n",
    "    self.container = bot.find_container()\n",
    "    book_list= self.container.find_elements(By.XPATH, './div')\n",
    "    link_list = []\n",
    "    for book in book_list:\n",
    "        link_list.append(book.find_element(By.TAG_NAME, 'a').get_attribute('href')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    def __init__(self, url: str = 'https://www.waterstones.com' ): \n",
    "        self.driver = webdriver.Chrome(service=s) \n",
    "        self.driver.get(url)\n",
    "\n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() \n",
    "        except TimeoutException: \n",
    "            print('No Cookies Found')\n",
    "    \n",
    "    def bestsellers(self, xpath: str='//*[@id=\"pagesmain\"]/div[3]/header[1]/a'):\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() \n",
    "        except TimeoutException: \n",
    "            print('Not Found')    \n",
    "\n",
    "    def find_container(self, xpath: str = '//div[@class=\"search-results-list\"]'): \n",
    "        return self.driver.find_element(By.XPATH, xpath)\n",
    "    \n",
    "    def extract_links(self):\n",
    "        self.container = bot.find_container()\n",
    "        book_list= self.container.find_elements(By.XPATH, './div')\n",
    "        self.link_list = []\n",
    "        for book in book_list:\n",
    "            self.link_list.append(book.find_element(By.TAG_NAME, 'a').get_attribute('href')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot = Scraper_project()\n",
    "bot.accept_cookies()\n",
    "bot.bestsellers()\n",
    "bot.find_container()\n",
    "bot.extract_links()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.link_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Rub the main body of the code only within if __name__ == \"__main__\" block "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    bot = Scraper_project()\n",
    "    bot.accept_cookies()\n",
    "    bot.bestsellers()\n",
    "    bot.find_container()\n",
    "    bot.extract_links()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next set of code extracts the weblinks from the first 3 pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_links_manypages(self):\n",
    "    link_lists = []\n",
    "    for i in range(0,3,1):\n",
    "        self.page_container = bot.driver.find_element(By.XPATH, '//div[@class=\"search-results-list\"]')\n",
    "        book_lists= self.page_container.find_elements(By.XPATH, './div')\n",
    "        for book in book_lists:\n",
    "            self.link_lists.append(book.find_element(By.TAG_NAME, 'a').get_attribute('href'))\n",
    "        #pagelinks.append(pagelink)\n",
    "        next = bot.driver.find_element(By.XPATH, '//a[@class=\"next \"]')\n",
    "        next.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    def __init__(self, url: str = 'https://www.waterstones.com' ): \n",
    "        self.driver = webdriver.Chrome(service=s) \n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() \n",
    "        except TimeoutException: \n",
    "            print('No Cookies Found')\n",
    "    \n",
    "    def bestsellers(self, xpath: str='//*[@id=\"pagesmain\"]/div[3]/header[1]/a'):\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() \n",
    "        except TimeoutException: \n",
    "            print('Not Found')    \n",
    "    \n",
    "    def find_container(self, xpath: str = '//div[@class=\"search-results-list\"]'): \n",
    "        return self.driver.find_element(By.XPATH, xpath)\n",
    "    \n",
    "    def extract_links_manypages(self):\n",
    "        self.link_lists = []\n",
    "        for page in range(0,3):\n",
    "            self.page_container = bot.driver.find_element(By.XPATH, '//div[@class=\"search-results-list\"]')\n",
    "            book_lists= self.page_container.find_elements(By.XPATH, './div')\n",
    "            for book in book_lists:\n",
    "                self.link_lists.append(book.find_element(By.TAG_NAME, 'a').get_attribute('href'))\n",
    "            next = bot.driver.find_element(By.XPATH, '//a[@class=\"next \"]')\n",
    "            next.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a method to extract the image links of the books in the first three pages as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    bot = Scraper_project()\n",
    "    bot.accept_cookies()\n",
    "    bot.bestsellers()\n",
    "    bot.find_container()\n",
    "    bot.extract_links_manypages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.link_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Milestone 3: Retrieving data from the details page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Create a function to retrieve test and image data from a single details page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Deterministically generate a unique ID. We would be using the ISBN as the unique ID of each book in the book list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Generate a v4 UUID to act as the globally unique ID for each entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Extract Data and store in a dictionary which maps feature name to feature value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tasks 1 - 4 are lumped in the code below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below defines a method to extract relevant data about the lists on the best seller page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: This first set of code extracts the details for just one page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    def __init__(self, url: str = 'https://www.waterstones.com' ): \n",
    "        self.driver = webdriver.Chrome(service=s) \n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() \n",
    "        except TimeoutException: \n",
    "            print('No Cookies Found')\n",
    "    \n",
    "    def bestsellers(self, xpath: str='//*[@id=\"pagesmain\"]/div[3]/header[1]/a'):\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "        except TimeoutException: \n",
    "            print('Not Found')    \n",
    "\n",
    "    def find_container(self, xpath: str = '//div[@class=\"search-results-list\"]'): \n",
    "        return self.driver.find_element(By.XPATH, xpath)\n",
    "\n",
    "    def extract_data(self):\n",
    "        self.book_dict = {\n",
    "            'ID':[],\n",
    "            'Link': [],\n",
    "            'src': [],\n",
    "            'Title': [],\n",
    "            'Author': [],\n",
    "            'Old Price': [],\n",
    "            'New Price': [],\n",
    "            'Listing Type': [],\n",
    "            'No_Reviews':[],\n",
    "            'Ratings': [] \n",
    "            }\n",
    "        \n",
    "        self.container = bot.find_container()\n",
    "        book_list= self.container.find_elements(By.XPATH, './div')\n",
    "        self.link_list = []\n",
    "        for book in book_list:\n",
    "            self.link_list.append(book.find_element(By.TAG_NAME, 'a').get_attribute('href')) \n",
    "            \n",
    "        for link in self.link_list[0:]:\n",
    "            id = str(uuid.uuid4())\n",
    "            self.book_dict['ID'].append(id) \n",
    "            self.driver.get(link) \n",
    "            time.sleep(1)\n",
    "            self.book_dict['Link'].append(link)\n",
    "            try:\n",
    "                src_path = bot.driver.find_elements(By.XPATH, '//div[@class=\"book-image-main\"]')\n",
    "                for path in src_path:\n",
    "                    src = path.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "                    self.book_dict['src'].append(src)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['src'].append('N/A')\n",
    "            try:\n",
    "                title = self.driver.find_element(By.XPATH, '//div[@class=\"title-wrap\"]/a')\n",
    "                self.book_dict['Title'].append(title.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Title'].append('N/A')\n",
    "            try:\n",
    "                author = self.driver.find_element(By.XPATH, '//span[@itemprop=\"author\"]')\n",
    "                self.book_dict['Author'].append(author.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Author'].append('N/A')\n",
    "            try:\n",
    "                old_Price = self.driver.find_element(By.XPATH, '//span[@class=\"price-rrp\"]')\n",
    "                self.book_dict['Old Price'].append(old_Price.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Old Price'].append('N/A')\n",
    "            try:\n",
    "                new_Price = self.driver.find_element(By.XPATH, '//span[@class=\"price\"]')\n",
    "                self.book_dict['New Price'].append(new_Price.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['New Price'].append('N/A')\n",
    "            try:\n",
    "                listing_Type = self.driver.find_element(By.XPATH, '//span[@itemprop=\"bookFormat\"]')\n",
    "                self.book_dict['Listing Type'].append(listing_Type.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Listing Type'].append('N/A')\n",
    "            try:\n",
    "                reviews = self.driver.find_element(By.XPATH, '//div[@class=\"star-rating\"]')\n",
    "                rev = reviews.find_element(By.XPATH, '//a[@class=\"reviews-trigger\"]')\n",
    "                self.book_dict['No_Reviews'].append(rev.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['No_Reviews'].append('N/A')\n",
    "            try:\n",
    "                ratings = bot.driver.find_elements(By.XPATH, '//div[@class=\"star-rating\"]')\n",
    "                for rating in ratings:\n",
    "                   try:\n",
    "                        full_star = rating.find_elements(By.XPATH, '//span[@class=\"star-icon full\"]')\n",
    "                        len_full_star = len(full_star)\n",
    "                   except NoSuchElementException:\n",
    "                        len_full_star = 0\n",
    "                   try:\n",
    "                        half_star = rating.find_elements(By.XPATH, '//span[@class=\"star-icon half\"]')\n",
    "                        len_half_star = len(half_star)\n",
    "                   except NoSuchElementException:\n",
    "                        len_half_star = 0\n",
    "                   try:\n",
    "                        no_star = rating.find_elements(By.XPATH, '//span[@class=\"star-icon\"]')\n",
    "                        len_no_star = len(no_star)\n",
    "                   except NoSuchElementException:\n",
    "                        len_no_star = 0\n",
    "                    \n",
    "                self.book_dict['Ratings'].append(len_full_star+(len_half_star/2)-len_no_star)\n",
    "                    \n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Ratings'].append('N/A')\n",
    "        return self.book_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    bot = Scraper_project()\n",
    "    bot.accept_cookies()\n",
    "    bot.bestsellers()\n",
    "    bot.find_container()\n",
    "    bot.extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.book_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a method to extract data for multiple pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scraper_project:\n",
    "    def __init__(self, url: str = 'https://www.waterstones.com' ): \n",
    "        self.driver = webdriver.Chrome(service=s) \n",
    "        self.driver.get(url)\n",
    "    \n",
    "    def accept_cookies(self, xpath: str='//button[@id=\"onetrust-accept-btn-handler\"]'):\n",
    "        time.sleep(5)\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click() \n",
    "        except TimeoutException: \n",
    "            print('No Cookies Found')\n",
    "    \n",
    "    def bestsellers(self, xpath: str='//*[@id=\"pagesmain\"]/div[3]/header[1]/a'):\n",
    "        try:\n",
    "            self.driver.find_element(By.XPATH, xpath).click()\n",
    "        except TimeoutException: \n",
    "            print('Not Found')    \n",
    "\n",
    "    def find_container(self, xpath: str = '//div[@class=\"search-results-list\"]'): \n",
    "        return self.driver.find_element(By.XPATH, xpath)\n",
    "\n",
    "    def extract_data(self):\n",
    "        self.book_dict = {\n",
    "            'ID':[],\n",
    "            'Link': [],\n",
    "            'src': [],\n",
    "            'Title': [],\n",
    "            'Author': [],\n",
    "            'Old Price': [],\n",
    "            'New Price': [],\n",
    "            'Listing Type': [],\n",
    "            'No_Reviews':[],\n",
    "            'Ratings': [] \n",
    "            }\n",
    "        \n",
    "        self.link_lists = []\n",
    "        for page in range(0,3):\n",
    "            self.page_container = bot.driver.find_element(By.XPATH, '//div[@class=\"search-results-list\"]')\n",
    "            book_lists= self.page_container.find_elements(By.XPATH, './div')\n",
    "            for book in book_lists:\n",
    "                self.link_lists.append(book.find_element(By.TAG_NAME, 'a').get_attribute('href'))\n",
    "            next = bot.driver.find_element(By.XPATH, '//a[@class=\"next \"]')\n",
    "            next.click()\n",
    "\n",
    "        for link in self.link_lists[0:]:\n",
    "            id = str(uuid.uuid4())\n",
    "            self.book_dict['ID'].append(id) \n",
    "            self.driver.get(link) \n",
    "            time.sleep(1)\n",
    "            self.book_dict['Link'].append(link)\n",
    "            try:\n",
    "                src_path = bot.driver.find_elements(By.XPATH, '//div[@class=\"book-image-main\"]')\n",
    "                for path in src_path:\n",
    "                    src = path.find_element(By.TAG_NAME, 'img').get_attribute('src')\n",
    "                    self.book_dict['src'].append(src)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['src'].append('N/A')\n",
    "            try:\n",
    "                title = self.driver.find_element(By.XPATH, '//div[@class=\"title-wrap\"]/a')\n",
    "                self.book_dict['Title'].append(title.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Title'].append('N/A')\n",
    "            try:\n",
    "                author = self.driver.find_element(By.XPATH, '//span[@itemprop=\"author\"]')\n",
    "                self.book_dict['Author'].append(author.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Author'].append('N/A')\n",
    "            try:\n",
    "                old_Price = self.driver.find_element(By.XPATH, '//span[@class=\"price-rrp\"]')\n",
    "                self.book_dict['Old Price'].append(old_Price.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Old Price'].append('N/A')\n",
    "            try:\n",
    "                new_Price = self.driver.find_element(By.XPATH, '//span[@class=\"price\"]')\n",
    "                self.book_dict['New Price'].append(new_Price.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['New Price'].append('N/A')\n",
    "            try:\n",
    "                listing_Type = self.driver.find_element(By.XPATH, '//span[@itemprop=\"bookFormat\"]')\n",
    "                self.book_dict['Listing Type'].append(listing_Type.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Listing Type'].append('N/A')\n",
    "            try:\n",
    "                reviews = self.driver.find_element(By.XPATH, '//div[@class=\"star-rating\"]')\n",
    "                rev = reviews.find_element(By.XPATH, '//a[@class=\"reviews-trigger\"]')\n",
    "                self.book_dict['No_Reviews'].append(rev.text)\n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['No_Reviews'].append('N/A')\n",
    "            try:\n",
    "                ratings = bot.driver.find_elements(By.XPATH, '//div[@class=\"star-rating\"]')\n",
    "                for rating in ratings:\n",
    "                   try:\n",
    "                        full_star = rating.find_elements(By.XPATH, '//span[@class=\"star-icon full\"]')\n",
    "                        len_full_star = len(full_star)\n",
    "                   except NoSuchElementException:\n",
    "                        len_full_star = 0\n",
    "                   try:\n",
    "                        half_star = rating.find_elements(By.XPATH, '//span[@class=\"star-icon half\"]')\n",
    "                        len_half_star = len(half_star)\n",
    "                   except NoSuchElementException:\n",
    "                        len_half_star = 0\n",
    "                   try:\n",
    "                        no_star = rating.find_elements(By.XPATH, '//span[@class=\"star-icon\"]')\n",
    "                        len_no_star = len(no_star)\n",
    "                   except NoSuchElementException:\n",
    "                        len_no_star = 0\n",
    "                    \n",
    "                self.book_dict['Ratings'].append(len_full_star+(len_half_star/2)-len_no_star)\n",
    "                    \n",
    "            except NoSuchElementException:\n",
    "                self.book_dict['Ratings'].append('N/A')\n",
    "        return self.book_dict\n",
    "                   \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    bot = Scraper_project()\n",
    "    bot.accept_cookies()\n",
    "    bot.bestsellers()\n",
    "    bot.find_container()\n",
    "    bot.extract_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot.book_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = bot.book_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Save the raw data dictionaries locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, write a code to create a folder called 'raw_data' in the root of your project folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing os module\n",
    "import os\n",
    "  \n",
    "# Directory\n",
    "directory = \"raw_data2\" \n",
    "parent_dir = \"C:/Users/preco/OneDrive/Desktop/AiCore/Web_Scraping\" \n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.mkdir(path)\n",
    "print(\"Directory '% s' created\" % directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The extracted data is saved to a json file called books.json. This will be saved in the raw_data folder we created earlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "    \n",
    "with open(\"./raw_data2/books_2.json\", \"w\") as outfile:\n",
    "    json.dump(books, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"images\" \n",
    "parent_dir = \"C:/Users/preco/OneDrive/Desktop/AiCore/Web_Scraping/raw_data2\" \n",
    "path = os.path.join(parent_dir, directory)\n",
    "os.mkdir(path)\n",
    "print(\"Directory '% s' created\" % directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_links = books['src']\n",
    "print(type(src_links))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for num, link_in_src_links in enumerate(src_links):\n",
    "    content = requests.get(link_in_src_links)\n",
    "    if content.status_code == 200:\n",
    "        with open('./raw_data2/images/image{}.png'.format(num), 'wb') as outfile:\n",
    "            outfile.write(content.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading the image data to my AWS S3 bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connection to the AWS bucket was made using the following configuration:\n",
    "\n",
    "* aws configure\n",
    "\n",
    " *  AWS Access Key ID [****************AGFC]: \n",
    " *  AWS Secret Access Key [****************hQvO]:\n",
    " *  Default region name [eu-west-2]: \n",
    " *  Default output format [json]: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "BUCKET_NAME = 'scraperbucket1000'\n",
    "FOLDER_NAME = 'images'\n",
    "\n",
    "session = boto3.Session(profile_name='default')\n",
    "s3 = session.client('s3')\n",
    "\n",
    "\n",
    "image_files = glob.glob(\"C:/Users/preco/OneDrive/Desktop/AiCore/Web_Scraping/raw_data2/images/image*.png\")\n",
    "\n",
    "for filename in image_files:\n",
    "    key = \"%s/%s\" % (FOLDER_NAME, os.path.basename(filename))\n",
    "    print(\"Putting %s as %s\" % (filename,key))\n",
    "    s3.upload_file(filename, BUCKET_NAME, key)\n",
    "\n",
    "print(\"All_Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "#s3.list_objects()\n",
    "\n",
    "#s3up = boto3.resource('s3')\n",
    "s3.upload_file(\"C:/Users/preco/OneDrive/Desktop/AiCore/Web_Scraping/cat_0.jpg\", \"scraperbucket1000\", \"catimage01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "DATABASE_TYPE = 'postgresql'\n",
    "DBAPI = 'psycopg2'\n",
    "HOST = '' # Change it to your AWS endpoint\n",
    "USER = 'p*****s'\n",
    "PASSWORD = 'a*******2'\n",
    "DATABASE = 'Books'\n",
    "PORT = 5432\n",
    "engine = create_engine(f\"{DATABASE_TYPE}+{DBAPI}://{USER}:{PASSWORD}@{HOST}:{PORT}/{DATABASE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df.to_sql('books_dataset', engine, if_exists='replace')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('presh')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fa2815679c5531b1099c20ba51474fe9611c18278d2ea95039fda6062886670f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
